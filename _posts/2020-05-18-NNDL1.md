---
layout: post
title: "《神经网络与深度学习》第一章学习笔记"
date: 2020-05-18 
description: "深度学习"
tag: 深度学习
---



### 《神经网络与深度学习》第一章学习笔记

深度学习是`机器学习`的一个分支，是指一类`问题`以及解决这类问题的`方法`.

首先，深度学习问题是一个机器学习问题，指从有限样例中通过算法总结出一般性的规律，并可以应用到新的未知数据上.

其次，深度学习采用的模型一般比较复杂，指样本的原始输入到输出目标之间的数据流经过多个线性或非线性的组件. 因为每个组件都会对信息进行加工，并进而影响后续的组件，所以当我们最后得到输出结果时，我们并不清楚其中每个组件的贡献是多少.  这个问题叫做贡献度分配问题（Credit Assignment Problem，CAP）`人工神经网络`（Artificial Neural Network，ANN)可以较好解决贡献度分配的问题.

神经网络一般比较复杂，从输入到输出的信息传递路径一般比较长，所以复杂神将网络的学习可以看成是一种深度的机器学习，即深度学习.

`神经网络和深度学习并不等价`.深度学习可以采用神经网络模型，也可以采用其他模型（比如深度信念网络是一种概率图模型）. 但是由于神经网络模型可以比较容易地解决贡献度分配问题，因此神经网络模型成为深度学习中主要采用的模型.

**1.1 人工智能**

简单来说，人工智能（artificial intelligence，AI）就是让机器具有人类的智慧，这也是人们长期追求的目标. 这里关于“智能”并没有一个明确的定义.

1950 年，阿兰·图灵（Alan Turing）发表了一篇有着重要影响力的论文《Computing Machinery and Intelligence》，讨论了创造一种“智能机器”的可能性. 由于“智能”一词比较难以定义，他提出了著名的图灵测试：“一个人在不接触对方的情况下，通过一种特殊的方式和对方进行一系列的问答. 如果在相当长时间内，他无法根据这些问题判断对方是人还是计算机，那么就可以认为这个计算机是智能的”.

和很多其他学科不同，人工智能这个学科的诞生有着明确的标志性事件，就是1956 年的达特茅斯（Dartmouth）会议.在这次会议上，“人工智能”被提出并作为本研究领域的名称. 同时，人工智能研究的使命也得以确定. John McCarthy （约翰·麦卡锡）提出了人工智能的定义：人工智能就是要让机器的行为看起来就像是人所表现出的智能行为一样.

目前，人工智能的主要领域大体上可以分为以下几个方面：

（1） `感知`：模拟人的感知能力，对外部刺激信息（视觉和语音等）进行感知和加工. 主要研究领域包括语音信息处理和计算机视觉等.

（2） `学习`：模拟人的学习能力，主要研究如何从样例或从与环境的交互中进行学习. 主要研究领域包括监督学习、无监督学习和强化学习等.

（3） `认知`：模拟人的认知能力，主要研究领域包括知识表示、自然语言理解、推理、规划、决策等.

***1.1.1 人工智能的发展历史***

人工智能从诞生至今，经历了一次又一次的繁荣与低谷，其发展历程大体上可以分为“推理期”、“知识期”和“学习期”

（1）推理期

1956 年达特茅斯会议之后，研究者对人工智能的热情高涨，之后的十几年是人工智能的黄金时期. 大部分早期研究者都通过人类的经验，基于逻辑或者事实归纳出来一些规则，然后通过编写程序来让计算机完成一个任务. 这个时期中，研究者开发了一系列的智能系统，比如几何定理证明器、语言翻译器等.

（2）知识期

到了20世纪70年代，研究者意识到`知识`对于人工智能系统的重要性. 在这一时期，出现了各种各样的专家系统（Expert System），并在特定的领域取得了很多成果. 专家系统可以简单理解为“知识库+推理机”，是一类具有专门知识和经验的计算机智能系统.专家系统也被称为`基于知识的系统`.专家系统的三要素：1）领域专家知识；2）模拟专家思维；3）达到专家级的水平

（3）学习期

对于人类的很多智能行为（比如语言理解、图像理解等），很难知道其中的原理，也无法描述这些智能行为背后的“知识”. 因此，很难通过知识和推理的方式来实现这些行为的智能系统. 为了解决这类问题，研究者开始将研究重点转向让计算机从数据中自己学习. 事实上，“学习”本身也是一种智能行为. 从人工智能的萌芽时期开始，就有一些研究者尝试让机器来自动学习，即`机器学习`（Machine Learning，ML）. 机器学习的主要目的是设计和分析一些`学习算法`，让计算机可以从数据（经验）中自动分析并获得规律，之后利用学习到的规律对未知数据进行预测，从而帮助人们完成一些特定任务，提高开发效率.机器学习的研究内容也十分广泛，涉及线性代数、概率论、统计学、数学优化、计算复杂性等多门学科．在人工智能领域，机器学习从一开始就是一个重要的研究方向. 但直到1980 年后，机器学习因其在很多领域的出色表现，才逐渐成为热门学科．

下图给出了人工智能发展历史上的重要事件.

![](/images/posts/ai1/pic1.png)

***1.1.2 人工智能的流派***

尽管人工智能的流派非常多，但主流的方法大体上可以归结为以下两种：

（1） 符号主义（Symbolism），又称逻辑主义、心理学派或计算机学派，是指通过分析人类智能的功能，然后用计算机来实现这些功能的一类方法.符号主义有两个基本假设：a）信息可以用符号来表示；b）符号可以通过显式的规则（比如逻辑运算）来操作.人类的认知过程可以看作符号操作过程. 在人工智能的推理期和知识期，符号主义的方法比较盛行，并取得了大量的成果.

（2） 连接主义（Connectionism），又称仿生学派或生理学派，是认知科学领域中的一类信息处理的方法和理论.在认知科学领域，人类的认知过程可以看作一种信息处理过程. 连接主义认为人类的认知过程是由大量简单神经元构成的神经网络中的信息处理过程，而不是符号运算. 因此，连接主义模型的主要结构是由大量简单的信息处理单元组成的互联网络，具有非线性、分布式、并行化、局部性计算以及自适应性等特性.

符号主义方法的一个优点是可解释性，而这也正是连接主义方法的弊端.深度学习的主要模型神经网络就是一种连接主义模型. 随着深度学习的发展，越来越多的研究者开始关注如何融合符号主义和连接主义，建立一种高效并且具有可解释性的模型.

**1.2 机器学习**

`机器学习`（machine learning，ML）是指从有限的观测数据中学习（或“猜测”）出具有一般性的规律，并利用这些规律对未知数据进行预测的方法. 机器学习是人工智能的一个重要分支，并逐渐成为推动人工智能发展的关键因素. 

![](/images/posts/ai1/pic2.png)

传统的机器学习主要关注如何学习一个`预测模型`．一般需要首先将数据表示为一组`特征`（Feature），特征的表示形式可以是连续的数值、离散的符号或其他形式．然后将这些特征输入到预测模型，并输出预测结果．这类机器学习可以看作`浅层学习`（Shallow Learning）．浅层学习的一个重要特点是不涉及特征学习，其特征主要靠人工经验或特征转换方法来抽取．特征的提取依靠经验，且是一个费时费力的过程.

在实际任务中使用机器学习模型一般会包含以下几个步骤：

（1） `数据预处理`：经过数据的预处理，如去除噪声等．比如在文本分类中，去除停用词等．

（2） `特征提取`：从原始数据中提取一些有效的特征．比如在图像分类中，提取边缘、尺度不变特征变换（Scale Invariant Feature Transform，SIFT）特征等．

（3） `特征转换`：对特征进行一定的加工，比如降维和升维. 很多特征转换方法也都是机器学习方法．降维包括`特征抽取`（Feature Extraction）和`特征选择`（Feature Selection）两种途径. 常用的特征转换方法有主成分分析（Principal Components Analysis，PCA）、线性判别分析（Linear Discriminant Analysis，LDA）等．

（4） `预测`：机器学习的核心部分，学习一个函数并进行预测．

![](/images/posts/ai1/pic3.png)

上述流程中，每步特征处理以及预测一般都是分开进行的. 传统的机器学习模型主要关注最后一步，即构建预测函数. 但是实际操作过程中，不同预测模型的性能相差不多，而前三步中的特征处理对最终系统的准确性有着十分关键的作用. 特征处理一般都需要人工干预完成，利用人类的经验来选取好的特征，并最终提高机器学习系统的性能. 因此，很多的机器学习问题变成了`特征工程`（Feature Engineering）问题. 开发一个机器学习系统的主要工作量都消耗在了预处理、特征提取以及特征转换上．

**1.3 表示学习**

为了提高机器学习系统的准确率，我们就需要将输入信息转换为有效的`特征`，或者更一般性地称为表示（Representation）. 如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作`表示学习`（Representation Learning）．我个人理解，表示学习中一个比较典型的方法就是自编码器（Auto-Encoder）.

表示学习的关键是解决`语义鸿沟`（Semantic Gap）问题。语义鸿沟问题是指输入数据的底层特征和高层语义信息之间的不一致性和差异性。比如给定一些关于“车”的图片，由于图片中每辆车的颜色和形状等属性都不尽相同，不同图片在像素级别上的表示（即底层特征）差异性也会非常大。但是我们人理解这些图片是建立在比较抽象的高层语义概念上的。如果一个预测模型直接建立在底层特征之上，会导致对预测模型的能力要求过高。如果可以有一个好的表示在某种程度上可以反映出数据的高层语义特征，那么我们就可以相对容易地构建后续的机器学习模型。

在表示学习中，有两个核心问题：一是“什么是一个好的表示？”；二是“如何学习到好的表示？”

好的表示没有明确的标准，但通常具有以下几个优点：

- 一个好的表示应该具有`很强的表示能力`，即同样大小的向量可以表示更多信息。
- 一个好的表示应该`使后续的学习任务变得简单`，即需要包含更高层的语义信息。
- 一个好的表示应该`具有一般性，是任务或领域独立的`。虽然目前的大部分表示学习方法还基于某个任务来学习，但我们期望学到的表示可以比较容易迁移到其它任务上。

在传统机器学习中，我们经常使用两种方式来表示特征：`局部表示`（Local Representation）和`分布式表示`（Distributed Representation）。

以颜色表示为例，一种表示颜色的方法是以不同名字来命名不同的颜色，这种表示方式叫做局部表示，也称为离散表示或符号表示。局部表示通常可以表示为`one-hot`向量的形式。假设有$v$种颜色，我们可以用一个大小为$1 \times v$的one-hot向量表示每种颜色。在第$i$种颜色对应的one-hot向量中，第$i$维的值为1，其余为0.

局部表示有两个优点：1）这种离散的表示方式具有很好的解释性，有利于人工归纳和总结特征，并通过特征组合进行高效的特征工程；2）通过多种特征组合得到的表示向量通常是稀疏的二值向量，当用于线性模型时计算效率非常高．

局部表示有两个不足之处：1）one-hot 向量的**维数很高，且不能扩展**。如果有一种新的颜色，我们就需要增加一维来表示；2）不同颜色之间的**相似度**都为0，即我们无法知道“红色”和“中国红”的相似度要比“红色”和“黑色”的相似度要高。

另一种表示颜色的方法是用RGB值来表示颜色，不同颜色对应R、G、B三维空间中一个点，这种表示方式叫做分布式表示。分布式表示通常可以表示为低维的稠密向量。

和局部表示相比，分布式表示的表示能力要比局部表示强很多，分布式表示的向量维度一般都比较低。我们只需要用一个三维的稠密向量就可以表示所有颜色。并且分布式表示也很容易表示新的颜色名。此外，不同颜色之间的相似度也很容易计算。

![](/images/posts/ai1/pic4.png)

我们可以使用神经网络来将高维的局部表示空间$\mathbb{R}^{|\nu|}$映射到一个非常低维的分布式表示空间$\mathbb{R}^{d}$。在这个低维空间中，每个特征不再是坐标轴上的点，而是分散在整个低维空间中。

在机器学习中，这个过程也称为**嵌入（Embedding）**。**嵌入通常指将一个度量空间中的一些对象映射到另一个低维的度量空间中，并尽可能保持不同对象之间的拓扑关系**。比如自然语言中词的分布式表示，也经常叫做词嵌入。

